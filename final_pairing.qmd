

```{r}

if (!require(librarian)){  # Check if the 'librarian' package is not installed
  install.packages("librarian")  # Install the 'librarian' package if it's not installed
  library(librarian)  # Load the 'librarian' package
}

# librarian downloads, if not already downloaded, and reads in needed packages
librarian::shelf(tidyverse, here, DBI, odbc)  # Load necessary packages: 'tidyverse', 'here', 'DBI', and 'odbc'


```

```{r}
tryCatch({  # Try to execute the following code and catch errors if they occur
    db_connection <- DBI::dbConnect(odbc::odbc(),  # Establish a database connection using ODBC
                             Driver="SnowflakeDSIIDriver",  # Specify the ODBC driver for Snowflake
                             Server="hawaiianair.west-us-2.azure.snowflakecomputing.com",  # Set the server address
                             WAREHOUSE="DATA_LAKE_READER",  # Set the Snowflake warehouse to use
                             Database="ENTERPRISE",  # Specify the database name
                             UID= Sys.getenv("UID"),  # Get the user ID from environment variables (should be your email)
                             authenticator = "externalbrowser")  # Use external browser authentication
    print("Database Connected!")  # Print a success message if connected
    },
    error=function(cond) {  # Define error handling function
            print("Unable to connect to Database.")  # Print an error message if the connection fails
})

# Set search_path
dbExecute(db_connection, "USE SCHEMA CREW_ANALYTICS")  # Set the database schema to 'CREW_ANALYTICS'


```

```{r}


fetch_data <- function(query, var_name) {  # Define a function to fetch data from the database
  if (!exists(var_name, envir = .GlobalEnv)) {  # Check if the variable name already exists in the global environment
    assign(var_name, dbGetQuery(db_connection, query), envir = .GlobalEnv)  # Fetch data and assign it to the variable in the global environment
    message(paste("Data fetched and stored in", var_name))  # Print a message indicating data was fetched
  } else {
    message(paste(var_name, "already exists. Skipping database query."))  # Print a message if the variable already exists
  }
}



```


## Flight History
```{r}

#'2019-01-01' AND '2024-09-27';"

q_flighthistory <- "SELECT *
                    FROM CT_FLIGHT_HISTORY WHERE FLIGHT_DATE BETWEEN '2024-10-29' AND '2024-11-30';"  # Define SQL query to select flight history between specified dates

fetch_data(q_flighthistory, "view_flighthistory")  # Fetch the flight history data and store it in 'view_flighthistory'

# Unique flight history for that date
clean_flighthistory <- view_flighthistory %>%  # Start processing the fetched flight history
  filter(SEGMENT_STATUS == "A") %>%  # Filter for active segments
  #select(c(2:10), (12:13), (55:59),EQUIPMENT) %>% # comment out
  rename(SCHED_DEPARTURE_TIME = SCHED_DEPARTURE_TIME_RAW) %>%  # Rename the departure time column
  mutate(updated_dt = paste(UPDATE_DATE, UPDATE_TIME, sep= " ")) %>%  # Create a new datetime column combining update date and time
  group_by(FLIGHT_NO, FLIGHT_DATE, DEPARTING_CITY, ARRIVAL_CITY, SCHED_DEPARTURE_TIME) %>%  # Group by multiple columns
  filter(updated_dt == max(updated_dt)) %>%  # Keep only the latest update for each group
  mutate(temp_id = cur_group_id()) %>%  # Create a temporary ID for each group
  filter(!duplicated(temp_id)) #%>%  # Remove duplicate rows based on temporary ID
  # select(!c(12:16)) %>% # comment out
  # select(!c(5:7)) %>%  # comment out
  # select(!c(10:11)) # comment out

rm(view_flighthistory)  # Remove the 'view_flighthistory' variable to free up memory
gc()  # Trigger garbage collection to clean up memory


```

## Flight leg
```{r}
#'2018-12-25' AND '2024-09-27';"

q_flightleg <- "SELECT *
                FROM CT_FLIGHT_LEG WHERE PAIRING_DATE BETWEEN '2024-10-26' AND '2024-11-30';"  # Define SQL query to select flight leg data for specified dates

fetch_data(q_flightleg, "view_flightleg")  # Fetch flight leg data and store it in 'view_flightleg'

flight_leg_join <-  view_flightleg %>%  # Start processing the fetched flight leg data
  mutate(updated_dt = paste(UPDATE_DATE, UPDATE_TIME, sep= " "),  # Create a new datetime column
         shed_dept_hour = hour(SCHED_DEPARTURE_TIME),  # Extract the hour from the scheduled departure time
         ) %>%
  relocate(updated_dt, .after=PAIRING_NO) %>%  # Move updated_dt column after PAIRING_NO
  group_by(PAIRING_POSITION,
           DEPARTING_CITY,
           ARRIVAL_CITY,
           SCHED_DEPARTURE_DATE,
           shed_dept_hour,
           PAIRING_NO,
           PAIRING_DATE) %>%  # Group by multiple columns
  filter(is.na(DEADHEAD),  # Keep only rows where DEADHEAD is NA
         updated_dt == max(updated_dt)) %>%  # Keep only the latest update for each group
  mutate(temp_id = cur_group_id(), .after = CREW_INDICATOR) %>%  # Create a temporary ID for each group
  filter(!duplicated(temp_id)) #%>%  # Remove duplicate rows based on temporary ID
  # ungroup() %>% # comment out
  # select(PAIRING_NO, PAIRING_DATE, FLIGHT_NO, SCHED_DEPARTURE_DATE, SCHED_ARRIVAL_DATE, DEPARTING_CITY, ARRIVAL_CITY, # Comment out
  #       PAIRING_POSITION, SCHED_BLOCK, EST_BLOCK, ACT_BLOCK_RAW, SCHED_BLOCK_CO, EST_BLOCK_CO, ACT_BLOCK_CO)

rm(view_flightleg)  # Remove the 'view_flightleg' variable to free up memory
gc()  # Trigger garbage collection to clean up memory

```


## Join flight history on flight leg
```{r}

correct_fh <- clean_flighthistory %>%  # Start with cleaned flight history
  inner_join(flight_leg_join, by = join_by(FLIGHT_NO, DEPARTING_CITY, ARRIVAL_CITY, 
                                           SCHED_DEPARTURE_DATE, SCHED_ARRIVAL_DATE),  # Perform an inner join with flight leg data based on specified columns
            relationship = "many-to-many")  # Specify the relationship type for the join


```

## Crew_id by pairing
```{r}

# MasterPairing Query
q_masterpairing <- "SELECT *
                    FROM CT_MASTER_PAIRING WHERE PAIRING_DATE BETWEEN '2024-10-26' AND '2024-11-30';"  # Define SQL query to select master pairing data

fetch_data(q_masterpairing, "view_masterpairing")  # Fetch master pairing data and store it in 'view_masterpairing'


### Rigid master pairing logic to lock in 1CA, 1FO, and 1RO per seat to fix "False Actives"
join_masterpairing <- view_masterpairing %>%  # Start processing the fetched master pairing data
  mutate(updated_dt = paste(UPDATE_DATE, UPDATE_TIME, sep = " ")) %>%  # Create a new datetime column
  group_by(PAIRING_DATE, CREW_ID) %>%  # Group by pairing date and crew ID
  filter(updated_dt == max(updated_dt),
          PAIRING_STATUS == "A"  # Filter for active pairings
        ) %>%
  ungroup() %>%  # Ungroup to return to a flat data frame
  group_by(PAIRING_DATE, PAIRING_NO, PAIRING_POSITION) %>%  # Group again for further filtering
  filter(
    # For CA, FO, and RO, keep only the employee with the latest updated_dt
    (PAIRING_POSITION %in% c("CA", "FO", "RO") & updated_dt == max(updated_dt)) |  # Keep the latest for specific positions
    # For FA, allow multiple employees
    (PAIRING_POSITION %in% c("FA", "JL", "KS", "JS", "LP", "KL"))  # Allow multiple for flight attendants
  ) %>%
  ungroup()  # Ungroup to return to a flat data frame


# ### Loose master pairing logic that contains "False Acrives"
# join_masterpairing <- view_masterpairing %>%
#     # Create a new column 'updated_dt' by concatenating 'UPDATE_DATE' and 'UPDATE_TIME'
#     mutate(updated_dt = paste(UPDATE_DATE, UPDATE_TIME, sep = " ")) %>%
#     
#     # Move the 'updated_dt' column to be immediately after the 'PAIRING_NO' column
#     relocate(updated_dt, .after = PAIRING_NO) %>%
#     
#     # Group the data by 'CREW_ID' and 'PAIRING_DATE'
#     group_by(CREW_ID, PAIRING_DATE) %>%
#     
#     # Keep only rows where 'updated_dt' is the latest for each group and
#     # the 'PAIRING_STATUS' is "A"
#     filter(updated_dt == max(updated_dt),
#            PAIRING_STATUS == "A")
 #%>%
  # select(PAIRING_POSITION, CREW_ID, PAIRING_NO, PAIRING_DATE, CREW_INDICATOR, updated_dt) %>% # comment out
  # ungroup()

  
rm(view_masterpairing)  # Remove the 'view_masterpairing' variable to free up memory
gc()  # Trigger garbage collection to clean up memory


```


## 99% Final Pairing - int island problem
```{r}


int_prob_final_pairing <- correct_fh %>%  # Start with correctly joined flight history
  inner_join(join_masterpairing, by = join_by(PAIRING_NO, PAIRING_DATE, PAIRING_POSITION),  # Perform an inner join with master pairing data based on specified columns
            relationship = "many-to-many") %>%  # Specify the relationship type for the join
  relocate(c(PAIRING_NO:CREW_ID), .before = FLIGHT_NO) %>%  # Move columns before FLIGHT_NO
  #drop_na(CREW_ID) %>%  # Commented out: would drop rows with NA in CREW_ID
  filter(!CREW_ID %in% c("6", "8", "10", "11", "35", "21", "7", "18", "1", "2", "3",  # Exclude specific crew IDs
                         "4", "5", "9", "12", "13", "14", "15", "17", "19", "20", "25",
                         "31", "32", "33", "34", "36", "37")) %>%  # Continue excluding specific crew IDs
  group_by(PAIRING_POSITION, PAIRING_NO, DEPARTING_CITY, ARRIVAL_CITY, SCHED_DEPARTURE_DATE, SCHED_DEPARTURE_TIME.x, PAIRING_DATE,  # Group by multiple columns
           CREW_ID) %>% 
  mutate(temp_id = cur_group_id()) %>%  # Create a temporary ID for each group
  filter(!duplicated(temp_id))  # Remove duplicate rows based on temporary ID


```



## Get Base from master_schedule
```{r}
# MasterPairing Query
q_masterschedule <- "SELECT *
                    FROM CT_MASTER_SCHEDULE WHERE BID_DATE BETWEEN '2018-12' AND '2024-09';"  # Define SQL query to select master schedule data

fetch_data(q_masterschedule, "view_masterschedule")  # Fetch master schedule data and store it in 'view_masterschedule'

clean_base <- view_masterschedule %>%  # Start processing the fetched master schedule data
  select(CREW_ID, BID_DATE, BASE) %>%  # Select relevant columns
  distinct()  # Remove duplicate rows


```


## Conditional func for int island problem - Final Pairing
```{r}

final_pairing <- int_prob_final_pairing %>%  # Start with final pairing data
  mutate(filter_group = case_when(EQUIPMENT == "717" & CREW_INDICATOR == "P" ~ "int_717_p",  # Categorize based on equipment and crew indicator
                                  EQUIPMENT == "717" & CREW_INDICATOR == "FA" ~ "fa_717",
                                  TRUE ~ "all_other_craft")) %>%  # Default case for all other equipment
  group_by(PAIRING_POSITION, PAIRING_NO, DEPARTING_CITY, 
           ARRIVAL_CITY, SCHED_DEPARTURE_DATE, 
           SCHED_DEPARTURE_TIME.x, PAIRING_DATE,  # Group by multiple columns
           filter_group) %>%
  filter(
    (filter_group == "int_717_p" & updated_dt == max(updated_dt)) |  # Keep only the latest update for specific group
    (filter_group != "int_717_p")) %>%
  ungroup() %>%  # Ungroup to return to a flat data frame
  select(!c(filter_group, temp_id)) %>%  # Remove unwanted columns
  group_by(PAIRING_NO, PAIRING_DATE, PAIRING_POSITION, CREW_ID, FLIGHT_NO, FLIGHT_DATE.x, DEPARTING_CITY,  # Group by multiple columns
           ARRIVAL_CITY, SCHED_DEPARTURE_DATE) %>% 
  mutate(temp_id = cur_group_id()) %>%  # Create a temporary ID for each group
  filter(!duplicated(temp_id)) %>%  # Remove duplicate rows based on temporary ID
  select(!c(temp_id, updated_dt)) %>%  # Remove unwanted columns
  select(!c(CREW_INDICATOR.x, updated_dt.y, updated_dt.x, SCHED_DEPARTURE_TIME.y,  # Remove additional unwanted columns
            SCHED_DEPARTURE_GMT_VAR.y, SCHED_ARRIVAL_TIME.y, SCHED_ARRIVAL_GMT_VAR.y, UPDATED_BY.y, UPDATED_BY.x,
            UPDATE_DATE.y, UPDATE_DATE.x, UPDATE_TIME.y, UPDATE_TIME.x, temp_id.x, temp_id.y, UNIQUE_ID,
            DEADHEAD, FLIGHT_DATE.x, UPDATED_BY, UPDATE_DATE, UPDATE_TIME, CREW_INDICATOR)) %>% 
  ungroup() %>%  # Ungroup to return to a flat data frame
  select(!FLIGHT_DATE.x) %>%  # Remove FLIGHT_DATE.x column
  rename(SCHED_DEPARTURE_TIME=SCHED_DEPARTURE_TIME.x,  # Rename columns for clarity
         SCHED_DEPARTURE_GMT_VAR=SCHED_DEPARTURE_GMT_VAR.x,
         SCHED_ARRIVAL_TIME = SCHED_ARRIVAL_TIME.x,
         SCHED_ARRIVAL_GMT_VAR = SCHED_ARRIVAL_GMT_VAR.x,
         FLIGHT_DATE=FLIGHT_DATE.y,
         CREW_INDICATOR=CREW_INDICATOR.y) %>% 
  relocate(c(CREW_ID, CREW_INDICATOR), .after = PAIRING_DATE) %>%  # Relocate columns for organization
  relocate(c(PAIRING_POSITION, FLIGHT_NO, FLIGHT_DATE, DEPARTING_CITY, ARRIVAL_CITY, SCHED_DEPARTURE_DATE, SCHED_DEPARTURE_TIME,  # Further relocate columns
             SCHED_ARRIVAL_DATE, SCHED_ARRIVAL_TIME, EQUIPMENT, EQUIPMENT_CODE, FIN_NO), .after=PAIRING_DATE) %>% 
  mutate(BID_DATE = format(PAIRING_DATE, "%Y-%m")) %>%  # Create a new BID_DATE column based on PAIRING_DATE
  left_join(clean_base, by = c("CREW_ID", "BID_PERIOD" = "BID_DATE")) %>%  # Join with cleaned base data
  relocate(BASE, .before = CREW_ID)  # Relocate BASE column before CREW_ID

Cols_AllMissing <- function(final_pairing) {  # Define a helper function to find columns with all missing values
  as.vector(which(colSums(is.na(final_pairing)) == nrow(final_pairing)))  # Return indices of columns that are entirely NA
}

final_pairing <- final_pairing %>%  # Start with final pairing data
  select(-Cols_AllMissing(.))  # Remove columns that are entirely NA

# t <- final_pairing %>%  # Create a summary table
#   group_by(FLIGHT_DATE, PAIRING_POSITION, FLIGHT_NO) %>%  # Group by flight date, pairing position, and flight number
#   reframe(n = n())  # Count the number of occurrences in each group

```
```{r}

t <- final_pairing %>% 
  filter(CREW_INDICATOR == "P") %>% 
  group_by(FLIGHT_DATE, FLIGHT_NO, DEPARTING_CITY, ARRIVAL_CITY, SCHED_DEPARTURE_TIME, SCHED_ARRIVAL_TIME) %>% 
  reframe(n = n(),
          PAIRING_NO = PAIRING_NO) %>% 
  filter(n <= 1)

```


## Search for codes to remove false actives - Attempt to remove false actives by looking at drop codes from master history
```{r}
# 
# master_history_q <- "select CREW_INDICATOR, CREW_ID, BID_PERIOD, TRANSACTION_CODE, PAIRING_NO, 
#                      PAIRING_DATE, TO_DATE, PAIRING_POSITION, PRIOR_TXN_CODE,
#                      UPDATE_DATE, UPDATE_TIME
#                      FROM CT_MASTER_HISTORY
#                      WHERE PAIRING_DATE BETWEEN '2024-07-25' AND '2024-08-31';"
# 
# fetch_data(master_history_q, 'view_masterhistory')
# 
# false_active_emp <- view_masterhistory %>% 
#   mutate(updated_dt = paste(UPDATE_DATE, UPDATE_TIME, sep= " ")) %>% 
#   filter() %>% 
#   group_by(CREW_ID, PAIRING_POSITION, PAIRING_DATE) %>% 
#   filter(updated_dt == max(updated_dt)) %>% 
#   #mutate(temp_id = cur_group_id()) %>% 
#   filter(#!duplicated(temp_id),
#          TRANSACTION_CODE %in% c("DRP", "TTD", "1SK", "PAY", "TSD", "REM", "VC2", "FAR", "OEP", "5SK",
#                                      "NQM", "FLP", "VC4"))
# 
# rm(view_masterhistory)
# gc()

```

## Test anti join
```{r}

t <- inner_join(false_active_emp, final_pairing, by = c("PAIRING_DATE", "PAIRING_POSITION", "CREW_ID", "PAIRING_NO")) %>% 
  group_by(PAIRING_POSITION, CREW_ID, PAIRING_DATE, PAIRING_NO) %>% 
  mutate(temp_id = cur_group_id()) %>% 
  filter(!duplicated(temp_id))

```


## loop to write table
```{r}

### Connect to `PLAYGROUND` Database
tryCatch({  # Try to execute the following code and catch errors if they occur
    db_connection_pg <- DBI::dbConnect(odbc::odbc(),  # Establish a new database connection using ODBC
                             Driver="SnowflakeDSIIDriver",  # Specify the ODBC driver for Snowflake
                             Server="hawaiianair.west-us-2.azure.snowflakecomputing.com",  # Set the server address
                             WAREHOUSE="DATA_LAKE_READER",  # Set the Snowflake warehouse to use
                             Database="PLAYGROUND",  # Specify the database name
                             UID= Sys.getenv("UID"),  # Get the user ID from environment variables
                             authenticator = "externalbrowser")  # Use external browser authentication
    print("Database Connected!")  # Print a success message if connected
    },
    error=function(cond) {  # Define error handling function
            print("Unable to connect to Database.")  # Print an error message if the connection fails
})

# Set search_path
dbExecute(db_connection_pg, "USE SCHEMA CREW_ANALYTICS")  # Set the database schema to 'CREW_ANALYTICS'

dbWriteTable(db_connection_pg, "AA_FINAL_PAIRING", final_pairing, overwrite = T)  # Write the final pairing data to the 'AA_FINAL_PAIRING' table, overwriting if it exists




```
